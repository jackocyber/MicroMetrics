% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
  landscape]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={ECON 21110 - Applied Microeconometrics - Assignment 1},
  pdfauthor={Jack Ogle},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{dcolumn}

\title{ECON 21110 - Applied Microeconometrics - Assignment 1}
\author{Jack Ogle}
\date{}

\begin{document}
\maketitle

Problem 1

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
\end{enumerate}

We then estimate the multiple regression for the following Models. See
Table 1 for Regression Results.

\[
(M1) \hspace{1cm} lwage_i = \beta_0 + \beta_1educ_i + \beta_3KWW_i + U_i 
\]

\[
(M2) \hspace{1cm} lwage_i = \beta_0 + \beta_1HS_i + \beta_2College_i + \beta_3KWW_i + U_i
\]

\begin{center} Explanation of (M1) regression results \end{center}

When \({educ_i}\) and \({KWW_i}\) are constant the mean hourly wage of
men ages 14-24 is 535.1 cents. Holding \({KWW_i}\) constant every year
increase of \({educ_i}\) there is a 2.1\% increase in hourly wage in
cents on average. Holding \({educ_i}\) constant every unit increase of
\({KWW_i}\) there is a 1.9\% increase in hourly wage in cents on
average.

\begin{center} Explanation of (M2) regression results \end{center}

When receiving both a high school or college education and KWW are
constant the mean hourly wage of men ages 14-24 is 553.3 cents. Holding
\({KWW_i}\) constant obtaining a high school education results in a
3.6\% increase in hourly wage in cents on average. Holding \({KWW_i}\)
constant obtaining a college education results in a 11.6\% increase in
hourly wage in cents on average. Holding both high school and college
education constant every unit increase of \({KWW_i}\) there is a 2.1\%
increase in hourly wage in cents on average.

\begin{table}[!htbp] \centering 
  \caption{Regression Results} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{lwage} \\ 
 & \multicolumn{1}{c}{M1} & \multicolumn{1}{c}{M2} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)}\\ 
\hline \\[-1.8ex] 
 educ & 0.021^{***} &  \\ 
  & (0.003) &  \\ 
  & & \\ 
 HS &  & 0.125^{***} \\ 
  &  & (0.022) \\ 
  & & \\ 
 College &  & 0.195^{***} \\ 
  &  & (0.026) \\ 
  & & \\ 
 KWW & 0.019^{***} & 0.019^{***} \\ 
  & (0.001) & (0.001) \\ 
  & & \\ 
 Constant & 5.351^{***} & 5.514^{***} \\ 
  & (0.039) & (0.031) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{2,963} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.196} & \multicolumn{1}{c}{0.199} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.195} & \multicolumn{1}{c}{0.199} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.396 (df = 2960)} & \multicolumn{1}{c}{0.395 (df = 2959)} \\ 
F Statistic & \multicolumn{1}{c}{360.893$^{***}$ (df = 2; 2960)} & \multicolumn{1}{c}{245.547$^{***}$ (df = 3; 2959)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{2}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

The OLS estimator \(\hat{\beta_1}\) is biased because it fails to
satisfy one of four necessary assumptions. The zero conditional mean
assumption does not hold.

\begin{center}
Zero Conditional Mean Assumption
\\
${E[U_i| educ_i, KWW_i] = 0}$
\end{center}

There are variables in the error term that are correlated with
independent variables. For example, if a parent of a child is educated,
then they would be more likely to have their child educated. If they are
educated then they would be more likely to have the required tuition
funds because they have a higher paying job. Additionally they would be
more likely to see the benefit of education as they benefited from the
connections and a higher paying job. Highly educated parents are likely
to produce highly educated offspring because they have the means and
wherewithal. This means that parents education a variable in \({U_i}\)
is correlated with our independent variable \({educ_i}\).

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\item
\end{enumerate}

In order for the Zero Conditional Mean Assumption to hold the variables
in \({U_i}\) cannot be correlated with the independent variables
\({educ_i}\) and \({KWW_i}\). To minimize the correlation between
\({KWW_i}\) and any variables in the error term we would want to measure
\({KWW_i}\) as early in the subjects as possible. This would minimize
the correlation because the child would not be exposed to varying levels
of education that might give some students and advantage over their
peers.

The study measured the participants \({KWW_i}\) as soon as the study
begins in 1966. This would mean that the youngest men would have been
age 14 and the oldest would have been age 24.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{3}
\item
\end{enumerate}

\[
(M3) \hspace{1cm} lwage_i = \beta_0 + \beta_1educ_i + \beta_2HS_i + \beta_3College_i + \beta_4KWW_i +  \beta_5HS_i*educ_i +  \beta_6College_i*educ_i + U_i
\]

(M2) is preferred to (M1) according to the beta coefficient values. If a
young man has received a College education then when education is
increased by one year there is a compensatory 4.6299\%
{[}100(\({\beta_1 + \beta_6)}\){]} increase in hourly wage. Compared to
if a young man has received a High School Education then when education
is increased by one year there is a 1.27\%
{[}100(\({\beta_1 + \beta_5)}\){]} decrease in the hourly wage. There is
a big difference between receiving a college education and receiving
only a High School education. This is because \({educ_i}\) is a
misleading explanatory value. It is misleading because their is such a
stark difference between the effects of college and high school on
wages. So we should exclude the variable \({educ_i}\) and use (M2). See
Table 2 for regression results.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{4}
\item
\end{enumerate}

For (M1) we see that the effect of education on wage per hour is smaller
at age 24 compared to the effect when considering all ages
\(hat{\beta_1}\). However when the men reach ages 28 and 32 we see that
beta estimator for those ages are larger than the beta estimator for All
ages. This is likely because the effect of education is lagging and it
takes a few years for the value of education to pay off and increase
wages. See table 3 for the regression results.

We see a similar trend in (M2) the effect of education on wage per hour
is less at age 24 compared to the effect when considering all ages. Then
men at age 28 and 32 see an increase in wage dependent on income. Both
\({\beta_1}\) and \({\beta_2}\) increase at these ages, so both High
School and College education increases income for these ages. See table
4 for the regression results.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{5}
\item
\end{enumerate}

When experience is held constant we see that \(hat{\beta_1}\) is less
than \({\beta_1}\). This can be explained by understanding what it means
to control for experience. Controlling for experience means that the
differences of experiences of individual subjects has no effect on the
wage of the subjects. Thus, the effect of getting an education has an
larger effect on wage. Keeping experience constant, more educated young
men have higher returns to education compared to less educated young
men. See table 5 for regression results.

In (M2) we see that the High School education coefficient in this
regression is less than the value we found in (a) at an experience level
of 4. And the same High School education coefficient was greater at
experience levels of 8 and 12 compared to our original (a) value. For
the college education coefficient we see that there is no value for
experience level 4. And for experience levels 8 and 12 they are greater
than the coefficient we originally found in (a). See table 6 for
regression results.

The effects of education are increasing in model two for the same
reasons that the increased in model one. It takes a few years for
individuals experience to pay off and effect wages. Additionally we see
that at a certain age and as experience increases less education matters
less with respect to increasing wages.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{6}
\item
\end{enumerate}

\[
lwage_i = \beta_0 + \beta_1educ_i + \beta_3KWW_i + \beta_4Exper_i + U_i
\]

Null Hypothesis: The coefficients for \({HS_i}\) and \({College_i}\)
from (M1) are equal to the coefficients in this model.

\[
lwage_i = \beta_0 + \beta_1HS_i + \beta_2College_i + \beta_3KWW_i + \beta_4Exper_i + U_i
\]

Null Hypothesis: The coefficients for \({HS_i}\) and \({College_i}\)
respectively from (M2) are equal to the coefficients in this model.

In our regression results found on table 7 we can reject our Null
Hypothesis because both \({\beta_1HS_i}\) and \({\beta_1HS_i}\) are
greater than the coefficient values we found in (a). When controlling
for experience there is a greater return to high school and college
education.

These models are more representative of the causal relationship between
educational degrees both high school and college and wages. Because
education and experience are correlated the more education and
individual obtains the less experience they can have.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{7}
\item
\end{enumerate}

\[
lwage_i = \beta_0 + \beta_1educ_i + \beta_3KWW_i + \beta_4Exper_i + \beta_5Exper_i*educ_i + U_i
\]

Null Hypothesis is that \({\beta_5}\) is 0.

We can reject this hypothesis if we look at the regression results and
see that it is not 0. It is 0.003.

\begin{enumerate}
\def\labelenumi{(\roman{enumi})}
\item
\end{enumerate}

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{9}
\item
\end{enumerate}

\[
lwage_i = \beta_0 + \beta_1educ_i + \beta_3KWW_i + \beta_4Black_i + U_i
\]

\[
lwage_i = \beta_0 + \beta_1HS_i + \beta_2College_i + \beta_3KWW_i + \beta_4Black_i + U_i
\]

Null Hypothesis is that \({\beta_4}\) is equal to 0. For Both Models.

We can clearly see that \({\beta_4}\) is not equal to 0 in both models.
Therefore we can reject our null hypothesis.

It is unlikely that there is a causal relationship between the
\({\beta_4Black_i}\) and wages. The zero conditional mean assumption is
violated. There is a variable in the error term that is correlated with
the independent variable of being black. Black people might be victims
of discrimination and not relieve the same education or job
opportunities compared to people of other races. This might be
correlated to lower wages.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{10}
\item
\end{enumerate}

\[
lwage_i = \beta_0 + \beta_1educ_i + \beta_3KWW_i + \beta_4Black_i + \beta_5Black_i*educ_i + U_i
\]

For (M1) the Null Hypothesis is that the \({\beta_5}\) is 0. Which we
can reject if we look at the regression results. Therefore education and
its returns to wages is variable depending on what race a individual is.

Often and unfortunately students and individuals of different races have
different access to education. Specifically, black students don't always
have access to good schools and education. Therefore the effect that
education has on their wages is correlated with their race.

\[
lwage_i = \beta_0 + \beta_1educ_i + \beta_3KWW_i + \beta_4Black_i + \beta_5Black_i*HS_i + \beta_5Black_i*College_i + U_i
\]

For (M2) the Null Hypothesis is that \({\beta_5}\) is 0 and
\({\beta_6}\) is 0

Because the estimator \({\beta_5}\) is not statistically significant we
cannot fully reject the Null hypothesis. However, we can reject part of
it as \({\beta_6}\) is not 0 and it is statistically significant. This
means that we cannot conclude anything about how high school education
might effect the returns to wage across different races.

However, because we reject the second half of the hypothesis. We can
conclude that a college education returns to wages does differ by race.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{11}
\item
\item
\end{enumerate}

Problem 2

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\item
\end{enumerate}

If we left out X2 we would be violating MLR.4. The zero conditional mean
assumption would be violated because it would then be placed in the
error term. Thus we would know that there is a variable that is
correlated with the independent variable income. If this key assumption
does not hold then there can be no causal effect of X1 on Y.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{1}
\item
\end{enumerate}

MLR.1-4 need to hold to for \(\hat{\beta_1}\) to be an unbiased and
consistent estimator. MLR.2 holds because we are pulling the data from a
random sample of 7000 households. MLR.3 holds because their is only one
independent variable so there is no other variable that could be
colinear. MLR.4 is not satisfied because there are variables in the
error term that are correlated with the independent variables. For
example the parents income is not included as an independent variable
and likely correlated with income. I don't find these conditions
credible because MLR.4 does not hold.

\begin{enumerate}
\def\labelenumi{(\alph{enumi})}
\setcounter{enumi}{2}
\item
\end{enumerate}

I would want to collect various other data that are related to income
and potentially correlated with income. I would want to do this in order
that the MLR.4 the zero conditional mean assumption holds. I would
collect income of the parents, spouse, and dependents.

Problem 3

\begin{itemize}
\tightlist
\item
  Standard of Living and Human Capital comparison among nations
\item
  A Contribution to the Empirics of Economic Growth Mankiw, Romer, Weil
\item
  Data comes from Real National Accounts which is publicly available
\item
  Uses OLS
\end{itemize}

\begin{table}[!htbp] \centering 
  \caption{Regression Results} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{3}{c}{\textit{Dependent variable:}} \\ 
\cline{2-4} 
\\[-1.8ex] & \multicolumn{3}{c}{lwage} \\ 
 & \multicolumn{1}{c}{M1} & \multicolumn{1}{c}{M2} & \multicolumn{1}{c}{M3} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)}\\ 
\hline \\[-1.8ex] 
 educ & 0.021^{***} &  & 0.025^{***} \\ 
  & (0.003) &  & (0.004) \\ 
  & & & \\ 
 HS &  & 0.036^{**} & 0.514^{***} \\ 
  &  & (0.016) & (0.129) \\ 
  & & & \\ 
 College &  & 0.116^{***} & -2.162^{***} \\ 
  &  & (0.026) & (0.745) \\ 
  & & & \\ 
 KWW & 0.019^{***} & 0.021^{***} & 0.018^{***} \\ 
  & (0.001) & (0.001) & (0.001) \\ 
  & & & \\ 
 educ:HS &  &  & -0.037^{***} \\ 
  &  &  & (0.010) \\ 
  & & & \\ 
 educ:College &  &  & 0.124^{***} \\ 
  &  &  & (0.042) \\ 
  & & & \\ 
 Constant & 5.351^{***} & 5.533^{***} & 5.288^{***} \\ 
  & (0.039) & (0.031) & (0.049) \\ 
  & & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{2,963} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.196} & \multicolumn{1}{c}{0.189} & \multicolumn{1}{c}{0.204} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.195} & \multicolumn{1}{c}{0.188} & \multicolumn{1}{c}{0.202} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.396 (df = 2960)} & \multicolumn{1}{c}{0.398 (df = 2959)} & \multicolumn{1}{c}{0.394 (df = 2956)} \\ 
F Statistic & \multicolumn{1}{c}{360.893$^{***}$ (df = 2; 2960)} & \multicolumn{1}{c}{229.850$^{***}$ (df = 3; 2959)} & \multicolumn{1}{c}{125.991$^{***}$ (df = 6; 2956)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{3}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{3}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{M1 Regression Results by Age} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{lwage} \\ 
 & \multicolumn{1}{c}{All Ages} & \multicolumn{1}{c}{Age = 24} & \multicolumn{1}{c}{Age = 28} & \multicolumn{1}{c}{Age = 32} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}\\ 
\hline \\[-1.8ex] 
 educ & 0.021^{***} & -0.0005 & 0.036^{***} & 0.043^{***} \\ 
  & (0.003) & (0.010) & (0.010) & (0.011) \\ 
  & & & & \\ 
 KWW & 0.019^{***} & 0.010^{***} & 0.011^{***} & 0.015^{***} \\ 
  & (0.001) & (0.003) & (0.003) & (0.004) \\ 
  & & & & \\ 
 Constant & 5.351^{***} & 5.791^{***} & 5.455^{***} & 5.319^{***} \\ 
  & (0.039) & (0.122) & (0.137) & (0.134) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{388} & \multicolumn{1}{c}{309} & \multicolumn{1}{c}{210} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.196} & \multicolumn{1}{c}{0.037} & \multicolumn{1}{c}{0.122} & \multicolumn{1}{c}{0.269} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.195} & \multicolumn{1}{c}{0.032} & \multicolumn{1}{c}{0.116} & \multicolumn{1}{c}{0.262} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.396 (df = 2960)} & \multicolumn{1}{c}{0.400 (df = 385)} & \multicolumn{1}{c}{0.380 (df = 306)} & \multicolumn{1}{c}{0.380 (df = 207)} \\ 
F Statistic & \multicolumn{1}{c}{360.893$^{***}$ (df = 2; 2960)} & \multicolumn{1}{c}{7.372$^{***}$ (df = 2; 385)} & \multicolumn{1}{c}{21.236$^{***}$ (df = 2; 306)} & \multicolumn{1}{c}{38.071$^{***}$ (df = 2; 207)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{4}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{M2 Regression Results by Age} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{lwage} \\ 
 & \multicolumn{1}{c}{All Ages} & \multicolumn{1}{c}{Age = 24} & \multicolumn{1}{c}{Age = 28} & \multicolumn{1}{c}{Age = 32} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}\\ 
\hline \\[-1.8ex] 
 HS & 0.125^{***} & 0.031 & 0.248^{***} & 0.412^{***} \\ 
  & (0.022) & (0.063) & (0.082) & (0.080) \\ 
  & & & & \\ 
 College & 0.195^{***} & -0.007 & 0.389^{***} & 0.479^{***} \\ 
  & (0.026) & (0.076) & (0.090) & (0.095) \\ 
  & & & & \\ 
 KWW & 0.019^{***} & 0.010^{***} & 0.010^{***} & 0.013^{***} \\ 
  & (0.001) & (0.003) & (0.003) & (0.004) \\ 
  & & & & \\ 
 Constant & 5.514^{***} & 5.766^{***} & 5.702^{***} & 5.608^{***} \\ 
  & (0.031) & (0.083) & (0.116) & (0.123) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{388} & \multicolumn{1}{c}{309} & \multicolumn{1}{c}{210} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.199} & \multicolumn{1}{c}{0.039} & \multicolumn{1}{c}{0.142} & \multicolumn{1}{c}{0.313} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.199} & \multicolumn{1}{c}{0.031} & \multicolumn{1}{c}{0.133} & \multicolumn{1}{c}{0.303} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.395 (df = 2959)} & \multicolumn{1}{c}{0.400 (df = 384)} & \multicolumn{1}{c}{0.376 (df = 305)} & \multicolumn{1}{c}{0.369 (df = 206)} \\ 
F Statistic & \multicolumn{1}{c}{245.547$^{***}$ (df = 3; 2959)} & \multicolumn{1}{c}{5.148$^{***}$ (df = 3; 384)} & \multicolumn{1}{c}{16.765$^{***}$ (df = 3; 305)} & \multicolumn{1}{c}{31.232$^{***}$ (df = 3; 206)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{4}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{M1 Regression Results by Experience} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{lwage} \\ 
 & \multicolumn{1}{c}{All Levels of Experience} & \multicolumn{1}{c}{Experience = 4} & \multicolumn{1}{c}{Experience = 8} & \multicolumn{1}{c}{Experience = 12} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}\\ 
\hline \\[-1.8ex] 
 educ & 0.021^{***} & 0.074^{***} & 0.052^{***} & 0.108^{***} \\ 
  & (0.003) & (0.026) & (0.012) & (0.019) \\ 
  & & & & \\ 
 KWW & 0.019^{***} & 0.018^{***} & 0.014^{***} & 0.009^{*} \\ 
  & (0.001) & (0.005) & (0.003) & (0.005) \\ 
  & & & & \\ 
 Constant & 5.351^{***} & 4.498^{***} & 5.063^{***} & 4.739^{***} \\ 
  & (0.039) & (0.369) & (0.121) & (0.162) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{163} & \multicolumn{1}{c}{308} & \multicolumn{1}{c}{132} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.196} & \multicolumn{1}{c}{0.208} & \multicolumn{1}{c}{0.283} & \multicolumn{1}{c}{0.452} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.195} & \multicolumn{1}{c}{0.198} & \multicolumn{1}{c}{0.278} & \multicolumn{1}{c}{0.444} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.396 (df = 2960)} & \multicolumn{1}{c}{0.384 (df = 160)} & \multicolumn{1}{c}{0.351 (df = 305)} & \multicolumn{1}{c}{0.366 (df = 129)} \\ 
F Statistic & \multicolumn{1}{c}{360.893$^{***}$ (df = 2; 2960)} & \multicolumn{1}{c}{21.024$^{***}$ (df = 2; 160)} & \multicolumn{1}{c}{60.154$^{***}$ (df = 2; 305)} & \multicolumn{1}{c}{53.227$^{***}$ (df = 2; 129)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{4}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{M2 Regression Results by Experience} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{lwage} \\ 
 & \multicolumn{1}{c}{All Levels of Experience} & \multicolumn{1}{c}{Experience = 4} & \multicolumn{1}{c}{Experience = 8} & \multicolumn{1}{c}{Experience = 12} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}\\ 
\hline \\[-1.8ex] 
 educ & 0.021^{***} &  &  &  \\ 
  & (0.003) &  &  &  \\ 
  & & & & \\ 
 College &  & 0.117^{*} & 0.362^{***} & 0.592^{***} \\ 
  &  & (0.069) & (0.088) & (0.136) \\ 
  & & & & \\ 
 HS &  &  & 0.160^{**} & 0.319^{***} \\ 
  &  &  & (0.067) & (0.088) \\ 
  & & & & \\ 
 KWW & 0.019^{***} & 0.021^{***} & 0.015^{***} & 0.016^{***} \\ 
  & (0.001) & (0.005) & (0.003) & (0.004) \\ 
  & & & & \\ 
 Constant & 5.351^{***} & 5.495^{***} & 5.540^{***} & 5.551^{***} \\ 
  & (0.039) & (0.149) & (0.092) & (0.125) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{163} & \multicolumn{1}{c}{308} & \multicolumn{1}{c}{132} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.196} & \multicolumn{1}{c}{0.183} & \multicolumn{1}{c}{0.278} & \multicolumn{1}{c}{0.412} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.195} & \multicolumn{1}{c}{0.172} & \multicolumn{1}{c}{0.271} & \multicolumn{1}{c}{0.398} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.396 (df = 2960)} & \multicolumn{1}{c}{0.391 (df = 160)} & \multicolumn{1}{c}{0.353 (df = 304)} & \multicolumn{1}{c}{0.381 (df = 128)} \\ 
F Statistic & \multicolumn{1}{c}{360.893$^{***}$ (df = 2; 2960)} & \multicolumn{1}{c}{17.861$^{***}$ (df = 2; 160)} & \multicolumn{1}{c}{39.020$^{***}$ (df = 3; 304)} & \multicolumn{1}{c}{29.861$^{***}$ (df = 3; 128)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{4}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{Regression Results for (g)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{lwage} \\ 
 & \multicolumn{1}{c}{M1} & \multicolumn{1}{c}{M2} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)}\\ 
\hline \\[-1.8ex] 
 educ & 0.056^{***} &  \\ 
  & (0.004) &  \\ 
  & & \\ 
 HS &  & 0.237^{***} \\ 
  &  & (0.024) \\ 
  & & \\ 
 College &  & 0.399^{***} \\ 
  &  & (0.033) \\ 
  & & \\ 
 KWW & 0.014^{***} & 0.015^{***} \\ 
  & (0.001) & (0.001) \\ 
  & & \\ 
 exper & 0.027^{***} & 0.022^{***} \\ 
  & (0.003) & (0.002) \\ 
  & & \\ 
 Constant & 4.820^{***} & 5.309^{***} \\ 
  & (0.063) & (0.037) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{2,963} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.225} & \multicolumn{1}{c}{0.223} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.225} & \multicolumn{1}{c}{0.222} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.389 (df = 2959)} & \multicolumn{1}{c}{0.389 (df = 2958)} \\ 
F Statistic & \multicolumn{1}{c}{286.844$^{***}$ (df = 3; 2959)} & \multicolumn{1}{c}{212.510$^{***}$ (df = 4; 2958)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{2}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{Regression Results for (h)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & \multicolumn{1}{c}{lwage} \\ 
\hline \\[-1.8ex] 
 educ & 0.031^{***} \\ 
  & (0.007) \\ 
  & \\ 
 KWW & 0.013^{***} \\ 
  & (0.001) \\ 
  & \\ 
 exper & -0.008 \\ 
  & (0.007) \\ 
  & \\ 
 educ:exper & 0.003^{***} \\ 
  & (0.001) \\ 
  & \\ 
 Constant & 5.164^{***} \\ 
  & (0.092) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.232} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.231} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.387 (df = 2958)} \\ 
F Statistic & \multicolumn{1}{c}{223.396$^{***}$ (df = 4; 2958)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{1}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{Regression Results for (j)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{lwage} \\ 
 & \multicolumn{1}{c}{M1} & \multicolumn{1}{c}{M2} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)}\\ 
\hline \\[-1.8ex] 
 educ & 0.020^{***} &  \\ 
  & (0.003) &  \\ 
  & & \\ 
 HS &  & 0.113^{***} \\ 
  &  & (0.021) \\ 
  & & \\ 
 College &  & 0.180^{***} \\ 
  &  & (0.025) \\ 
  & & \\ 
 KWW & 0.016^{***} & 0.016^{***} \\ 
  & (0.001) & (0.001) \\ 
  & & \\ 
 black & -0.133^{***} & -0.129^{***} \\ 
  & (0.019) & (0.019) \\ 
  & & \\ 
 Constant & 5.490^{***} & 5.640^{***} \\ 
  & (0.044) & (0.036) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{2,963} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.209} & \multicolumn{1}{c}{0.211} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.208} & \multicolumn{1}{c}{0.210} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.393 (df = 2959)} & \multicolumn{1}{c}{0.392 (df = 2958)} \\ 
F Statistic & \multicolumn{1}{c}{260.601$^{***}$ (df = 3; 2959)} & \multicolumn{1}{c}{198.149$^{***}$ (df = 4; 2958)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{2}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\begin{table}[!htbp] \centering 
  \caption{Regression Results for (k)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{lwage} \\ 
 & \multicolumn{1}{c}{M1} & \multicolumn{1}{c}{M2} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)}\\ 
\hline \\[-1.8ex] 
 HS &  & 0.084^{***} \\ 
  &  & (0.030) \\ 
  & & \\ 
 College &  & 0.138^{***} \\ 
  &  & (0.052) \\ 
  & & \\ 
 educ & 0.016^{***} & 0.002 \\ 
  & (0.003) & (0.007) \\ 
  & & \\ 
 black:educ &  & 0.017^{**} \\ 
  &  & (0.007) \\ 
  & & \\ 
 KWW & 0.016^{***} & 0.016^{***} \\ 
  & (0.001) & (0.001) \\ 
  & & \\ 
 black & -0.366^{***} & -0.345^{***} \\ 
  & (0.085) & (0.089) \\ 
  & & \\ 
 educ:black & 0.019^{***} &  \\ 
  & (0.007) &  \\ 
  & & \\ 
 Constant & 5.551^{***} & 5.656^{***} \\ 
  & (0.049) & (0.073) \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{2,963} & \multicolumn{1}{c}{2,963} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.211} & \multicolumn{1}{c}{0.213} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.210} & \multicolumn{1}{c}{0.212} \\ 
Residual Std. Error & \multicolumn{1}{c}{0.392 (df = 2958)} & \multicolumn{1}{c}{0.392 (df = 2956)} \\ 
F Statistic & \multicolumn{1}{c}{197.861$^{***}$ (df = 4; 2958)} & \multicolumn{1}{c}{133.532$^{***}$ (df = 6; 2956)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{2}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table}

\end{document}
