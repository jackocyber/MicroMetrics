---
title: "Assignment 5 Question 1"
author: "Jack Ogle in collaboration with Eva Haque, Matt Lohrs, and Jack Knickrehm"
output:
  pdf_document:
    keep_tex: true
header-includes:
   - \usepackage{dcolumn}
   - \usepackage{float}
classoption: landscape
geometry: margin=1in
fontsize: 12pt
---


```{r, include=FALSE}
# Loading Packages
library(tidyverse)
library(haven)
library(stargazer)
library(tinytex)
library(foreign)
library(multiwayvcov)
library(lmtest)
library(rdrobust)

# Loading Data
damon = read_dta('/Users/matthewogle/micro-metrics/Assignment5/damonclark.dta')

# Generating Dummy for winning and losing
damon$win = ifelse((damon$vote >= 50), 1, 0)
damon$lose = ifelse((damon$vote < 50), 1, 0)

# Generating Margin
damon$margin = damon$vote - 50
damon$margin_sqr = damon$margin^2

# Generating Interactions
win_margin = lm(win ~ margin + margin_sqr, data = damon)
lose_margin = lm(lose ~ margin + margin_sqr, data = damon)

```


(a)

The assumptions underlying the RDD in this paper are that in order for the school to attain autonomy (GM) there must be a 50% vote from the community to have the school be GM. Clark also assumes that the school performance, test scores, is increasing in autonomy and school effort. Schools that are already GM at the start of a period are fully autonomous and therefore decide only how much effort to exert. Effort in turn improves school performance, test scores for example, but effort is costly. Schools that are not GM at the start period must decided how much effort to exert and whether or not to become Gm. For given effort, non GM schools performance is assumed lower than GM school performance; hence schools have an incentive to become GM. There are cost associated with GM status and the decision to become one is non trivial.

The conceptual framework assumed that schools were identical. In practice, schools differ along many dimensions, and certain types of school may be more likely to hold and win a GM vote (e.g., those with more entrepreneurial head teachers). Clark's empirical approach overcomes this selection problem by focusing on the jump in performance among schools at the 50 percent win threshold. Specifically, Clark considers variants of the fuzzy regression discontinuity model for school i voting on GM

(b)

These figures represent figure 8 from the paper. They are all visualizing:

The Impact of GM Status on Schools that Become Grant-Maintained

```{r}
# Restricting Data to (15,85) 
restrict = subset(damon, vote <= 85 & vote >= 15)

# calculating the % change
restrict$percentage_change = restrict$passrate2 - restrict$passrate0

# Base Year
rdplot(y = restrict$passrate0, x = restrict$vote , c = 50, p = 3, nbins = 7, title = "Base Year (bin-width = 7)", x.label= "Vote Share", y.label = "% Pass")

# Base +2 Year
rdplot(y = restrict$passrate2, x = restrict$vote, c = 50, p = 3, nbins = 7, title = "Base + 2 Years (bin-width = 7)", x.label= "Vote Share", y.label = "% Pass")

# % change
rdplot(y = restrict$percentage_change, x = restrict$vote, p = 1, nbins = 7, c = 50, title = "Change (bin-width = 7)", x.label= "Vote Share", y.label = "% Pass")


# Base Year
rdplot(y = restrict$passrate0, x = restrict$vote , c = 50, p = 3, nbins = 2, title = "Base Year (bin-width = 2)", x.label= "Vote Share", y.label = "% Pass")

# Base +2 Year
rdplot(y = restrict$passrate2, x = restrict$vote, c = 50, p = 3, nbins = 2, title = "Base + 2 Years (bin-width = 2)", x.label= "Vote Share", y.label = "% Pass")

# % change
rdplot(y = restrict$percentage_change, x = restrict$vote, p = 1, nbins = 2, c = 50, title = "Change (bin-width = 2)", x.label= "Vote Share", y.label = "% Pass")


```

From the graphs above we can see that as you decrease the bin size from 7 which is what Clark uses in the paper for Figure 8. We can see that there is no difference in terms of the lines of best fit, however, we can better visualize the data with a higher bin width. We see more of the data points.

(c)

Clark restricts his sample to those schools with votes from 15% and 85% because he wants to reduce bias from outliers from schools who were very opposed or very for autonomy. For example schools with high vote shares may have been under threat of closure, whilst few schools received very low vote shares. Additionally, RD necessitates that in order to estimate the average effect of a treatment you look at an arbitrary cutoff and evaluate the treatment effects before and after the cutoff. Additionally they saw that schools outside this interval have different baseline characteristics and are less likely to survive. Clark needs to estimate around the cutoff, 50, which is inside the (15,85) interval. Other columns have functions of vote share because vote share is the forcing variable in this Fuzzy Regression Discontinuity model. Fuzzy RD exploits discontinuities in the probability of treatment conditional on a variate. The discontinuity, here is Vote share, and it becomes the IV for the treatment status. Which is why Table 3a includes functions of vote share both on their own and interacted with the win/lose variable.

(d)

```{r, echo=FALSE}
restrict$vote_share <- restrict$vote/100
restrict$vote_share_sqd <- restrict$vote_share^2

model_d_1 <- lm(passrate2 ~ win + vote_share + vote_share*lose + vote_share*win, data = restrict)

model_d_2 <- lm(passrate2 ~ vote_share + vote_share*lose + vote_share*win, data = restrict)

model_d_3 <- lm(passrate2 ~ win + vote_share + (vote_share^2)*lose + (vote_share^2)*win, data = restrict)

model_d_4 <- lm(passrate2 ~ win + vote_share + vote_share*lose + vote_share*win + (vote_share^2)*lose + (vote_share^2)*win, data = restrict)


# summary(model_d_1 <- rdrobust(restrict, restrict$win, c = 0.5, all=T))
# Non-Grammar Schools with Vote Shares in [15,85] interval

stargazer(model_d_1, model_d_2, model_d_3, model_d_4, title="Regression Results (d)", align=TRUE, notes = "Standard errors in parentheses", notes.align = "l")

```

\begin{table}[H] \centering 
  \caption{Regression Results (d)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{passrate2} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}\\ 
\hline \\[-1.8ex] 
 win & 14.727^{**} &  & 14.727^{**} & 14.727^{**} \\ 
  & (7.235) &  & (7.235) & (7.235) \\ 
  & & & & \\ 
 vote\_share & -17.141^{**} & -17.141^{**} & -17.141^{**} & -17.141^{**} \\ 
  & (8.134) & (8.134) & (8.134) & (8.134) \\ 
  & & & & \\ 
 lose &  & -14.727^{**} &  &  \\ 
  &  & (7.235) &  &  \\ 
  & & & & \\ 
 vote\_share* lose & 24.871^{*} & 24.871^{*} & 24.871^{*} & 24.871^{*} \\ 
  & (14.672) & (14.672) & (14.672) & (14.672) \\ 
  & & & & \\ 
 win*vote\_share &  &  &  &  \\ 
  &  &  &  &  \\ 
  & & & & \\ 
 vote\_share*win &  &  &  &  \\ 
  &  &  &  &  \\ 
  & & & & \\ 
 Constant & 40.597^{***} & 55.325^{***} & 40.597^{***} & 40.597^{***} \\ 
  & (4.348) & (5.782) & (4.348) & (4.348) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{524} & \multicolumn{1}{c}{524} & \multicolumn{1}{c}{524} & \multicolumn{1}{c}{524} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.009} & \multicolumn{1}{c}{0.009} & \multicolumn{1}{c}{0.009} & \multicolumn{1}{c}{0.009} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.004} & \multicolumn{1}{c}{0.004} & \multicolumn{1}{c}{0.004} & \multicolumn{1}{c}{0.004} \\ 
Residual Std. Error (df = 520) & \multicolumn{1}{c}{15.267} & \multicolumn{1}{c}{15.267} & \multicolumn{1}{c}{15.267} & \multicolumn{1}{c}{15.267} \\ 
F Statistic (df = 3; 520) & \multicolumn{1}{c}{1.614} & \multicolumn{1}{c}{1.614} & \multicolumn{1}{c}{1.614} & \multicolumn{1}{c}{1.614} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{4}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table} 

* I attempted to estimate the some regressions where vote was squared, but I got slightly different results. I attribute this to a lack of data. We lack the data that is required to obtain certain regression results. We cannot control for all the treatments and controls mentioned in the empirical section of the paper where it mentions the design behind each regression column. However, here is the design of all the columns: 

Column 1 describes the mean improvement difference between winners and losers, column 2 adds a control for vote share, and column 3 interacts vote share with win (the specification used to generate the fitted lines in Figure 8). In column 4 they weight according to the size of the school exam-taking cohort, to give more weight to schools with more exam-takers (to the extent the model is correctly specified). In column 5 Clark uses a quadratic vote share control function and the estimated impact of winning falls. This is not surprising given the degree of concavity in the (50,85) interval (Figure 8), although a cubic fit would be expected to pick up this shape (and the dip to the left of the 50% threshold). In the interests of parsimony, since the baseline levels are relatively flat and Clark have no priors regarding the appropriate functional form, Clark reverts to the linear specification. His preferred estimates are in column 6.

If the RDD is valid in this case the internal validity of the results would still be affected because there would be a risk of violating the zero conditional mean assumption. Because we don't have the correct data that we could use in our model to control the Ommitted variable bias. The variables that Clark includes reduce OVB because by including them in the model correlation between the error term and the independent variables decreases. 


(e)

```{r}
# Sampling smaller thresholds (95, 5)
restrict = subset(damon, vote <= 75 & vote >= 25)


restrict$vote_share <- restrict$vote/100
restrict$vote_share_sqd <- restrict$vote_share^2

model_d_1 <- lm(passrate2 ~ win + vote_share + vote_share*lose + vote_share*win, data = restrict)

model_d_2 <- lm(passrate2 ~ vote_share + vote_share*lose + vote_share*win, data = restrict)

model_d_3 <- lm(passrate2 ~ win + vote_share + (vote_share^2)*lose + (vote_share^2)*win, data = restrict)

model_d_4 <- lm(passrate2 ~ win + vote_share + vote_share*lose + vote_share*win + (vote_share^2)*lose + (vote_share^2)*win, data = restrict)

stargazer(model_d_1, model_d_2, model_d_3, model_d_4, title="Regression Results (e)", align=TRUE, notes = "Standard errors in parentheses", notes.align = "l")

```

\begin{table}[H] \centering 
  \caption{Regression Results (e)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} D{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{4}{c}{\textit{Dependent variable:}} \\ 
\cline{2-5} 
\\[-1.8ex] & \multicolumn{4}{c}{passrate2} \\ 
\\[-1.8ex] & \multicolumn{1}{c}{(1)} & \multicolumn{1}{c}{(2)} & \multicolumn{1}{c}{(3)} & \multicolumn{1}{c}{(4)}\\ 
\hline \\[-1.8ex] 
 win & 5.001 &  & 5.001 & 5.001 \\ 
  & (11.565) &  & (11.565) & (11.565) \\ 
  & & & & \\ 
 vote\_share & 3.169 & 3.169 & 3.169 & 3.169 \\ 
  & (14.864) & (14.864) & (14.864) & (14.864) \\ 
  & & & & \\ 
 lose &  & -5.001 &  &  \\ 
  &  & (11.565) &  &  \\ 
  & & & & \\ 
 vote\_share:lose & 10.491 & 10.491 & 10.491 & 10.491 \\ 
  & (22.888) & (22.888) & (22.888) & (22.888) \\ 
  & & & & \\ 
 win:vote\_share &  &  &  &  \\ 
  &  &  &  &  \\ 
  & & & & \\ 
 vote\_share:win &  &  &  &  \\ 
  &  &  &  &  \\ 
  & & & & \\ 
 Constant & 38.259^{***} & 43.260^{***} & 38.259^{***} & 38.259^{***} \\ 
  & (6.606) & (9.493) & (6.606) & (6.606) \\ 
  & & & & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{357} & \multicolumn{1}{c}{357} & \multicolumn{1}{c}{357} & \multicolumn{1}{c}{357} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.006} & \multicolumn{1}{c}{0.006} & \multicolumn{1}{c}{0.006} & \multicolumn{1}{c}{0.006} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{-0.003} & \multicolumn{1}{c}{-0.003} & \multicolumn{1}{c}{-0.003} & \multicolumn{1}{c}{-0.003} \\ 
Residual Std. Error (df = 353) & \multicolumn{1}{c}{15.577} & \multicolumn{1}{c}{15.577} & \multicolumn{1}{c}{15.577} & \multicolumn{1}{c}{15.577} \\ 
F Statistic (df = 3; 353) & \multicolumn{1}{c}{0.666} & \multicolumn{1}{c}{0.666} & \multicolumn{1}{c}{0.666} & \multicolumn{1}{c}{0.666} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{4}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{4}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table} 

* When I experimented with a threshold of smaller [25,75] I get slightly different results compared to my results previously from (d) and they also differ from Clark's results. We see a larger focus on the RD and therefore the effect of win the election decreases. And the average effect of the interaction between vote share and losing the election decreases. A likely reasons for this change is that the overall estimator slope changes with the reduction of observations. 

(f)

```{r}

restrict = subset(damon, vote <= 85 & vote >= 15)

model_f_1 <- lm(passrate0 ~ win, data = restrict )

stargazer(model_f_1, title="Regression Results (f)", align=TRUE, notes = "Standard errors in parentheses", notes.align = "l")

```
\begin{table}[!htbp] \centering 
  \caption{Regression Results (f)} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} } 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & \multicolumn{1}{c}{passrate0} \\ 
\hline \\[-1.8ex] 
 win & -2.082 \\ 
  & (1.364) \\ 
  & \\ 
 Constant & 41.462^{***} \\ 
  & (1.081) \\ 
  & \\ 
\hline \\[-1.8ex] 
Observations & \multicolumn{1}{c}{524} \\ 
R$^{2}$ & \multicolumn{1}{c}{0.004} \\ 
Adjusted R$^{2}$ & \multicolumn{1}{c}{0.003} \\ 
Residual Std. Error & \multicolumn{1}{c}{15.093 (df = 522)} \\ 
F Statistic & \multicolumn{1}{c}{2.329 (df = 1; 522)} \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{l}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
 & \multicolumn{1}{l}{Standard errors in parentheses} \\ 
\end{tabular} 
\end{table} 


(g)
(h)
(i)
(j)